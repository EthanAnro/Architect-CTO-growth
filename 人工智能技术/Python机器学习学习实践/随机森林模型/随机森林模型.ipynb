{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc9e8419",
   "metadata": {},
   "source": [
    "## 集成学习模型\n",
    "我们常说“团结就是力量”，集成学习采用的其实就是这一思想：将多个模型组合在一起，从而产生更强大的模型。随机森林模型就是典型的集成模型。\n",
    "\n",
    "集成学习模型使用一系列弱学习器（也称为基础模型或基模型）进行学习，并将各个弱学习器的结果进行整合，从而获得比单个学习器更好的学习效果。集成学习模型的常见算法有Bagging算法和Boosting算法两种。Bagging算法的典型机器学习模型为随机森林模型，而Boosting算法的典型机器学习模型为AdaBoost、GBDT、XGBoost和LightGBM模型。\n",
    "\n",
    "### 1、Bagging算法\n",
    "Bagging算法的原理类似投票，每个弱学习器都有一票，最终根据所有弱学习器的投票，按照“少数服从多数”的原则产生最终的预测结果。\n",
    "![Bagging算法](image/Bagging算法.png)\n",
    "\n",
    "### 2、Boosting算法\n",
    "Boosting算法的本质是将弱学习器提升为强学习器，它和Bagging算法的区别在于：Bagging算法对待所有的弱学习器一视同仁；而Boosting算法则会对弱学习器“区别对待”，通俗来讲就是注重“培养精英”和“重视错误”。\n",
    "\n",
    "“培养精英”就是每一轮训练后对预测结果较准确的弱学习器给予较大的权重，对表现不好的弱学习器则降低其权重。这样在最终预测时，“优秀模型”的权重是大的，相当于它可以投出多票，而“一般模型”只能投出一票或不能投票。\n",
    "\n",
    "“重视错误”就是在每一轮训练后改变训练集的权值或概率分布，通过提高在前一轮被弱学习器预测错误的样例的权值，降低前一轮被弱学习器预测正确的样例的权值，来提高弱学习器对预测错误的数据的重视程度，从而提升模型的整体预测效果。\n",
    "![Boosting算法](image/Boosting算法.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed52120",
   "metadata": {},
   "source": [
    "## 随机森林模型的基本原理\n",
    "随机森林（Random Forest）是一种经典的Bagging模型，其弱学习器为决策树模型。随机森林模型会在原始数据集中随机抽样，构成n个不同的样本数据集，然后根据这些数据集搭建n个不同的决策树模型，最后根据这些决策树模型的平均值（针对回归模型）或者投票情况（针对分类模型）来获取最终结果。\n",
    "![随机森林模型原理](image/随机森林模型原理.png)\n",
    "为了保证模型的泛化能力（或者说通用能力），随机森林模型在建立每棵树时，往往会遵循“**数据随机**”和“**特征随机**”这两个基本原则。\n",
    "\n",
    "与单独的决策树模型相比，随机森林模型由于集成了多个决策树，其预测结果会更准确，且不容易造成过拟合现象，泛化能力更强。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70760d01",
   "metadata": {},
   "source": [
    "## 随机森林模型的代码实现\n",
    "和决策树模型一样，随机森林模型既能进行分类分析，又能进行回归分析，对应的模型分别为随机森林分类模型（RandomForestClassifier）和随机森林回归模型（RandomForestRegressor）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3c907",
   "metadata": {},
   "source": [
    "### 1、随机森林分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085c467e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # 引入随机森林分类模型相关库RandomForestClassifier\n",
    "x = [[1,2], [3,4], [5,6], [7,8], [9,10]] # X是特征变量，共有2个特征\n",
    "y = [0, 0, 0, 1, 1] # y是目标变量，共有2个分类——0和1\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=123) # 引入模型，并设置弱学习器的数量n_estimators为10，即共有10个决策树模型作为弱学习器；设置random_state为123，使得每次运行结果一致（如果不设置则可能出现每次预测结果不一样的情况，这是因为随机森林遵循“数据随机”和“特征随机”的基本原则）\n",
    "model.fit(x, y) # 用fit()函数训练模型\n",
    "\n",
    "print(model.predict([[5,5]])) # 用predict()函数进行预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15051656",
   "metadata": {},
   "source": [
    "### 2、随机森林回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac27131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.8]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor # 引入随机森林回归模型相关库RandomForestRegressor\n",
    "x = [[1,2], [3,4], [5,6], [7,8], [9,10]] # X是特征变量，共有2个特征\n",
    "y = [1, 2, 3, 4, 5] # y是目标变量，它是一个连续值\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=10, random_state=123) # 引入模型，并设置弱学习器的数量n_estimators为10，即共有10个决策树模型作为弱学习器；设置random_state为123，使得每次运行结果一致\n",
    "model.fit(x, y) # 用fit()函数训练模型\n",
    "\n",
    "print(model.predict([[5,5]])) # 用predict()函数进行预测"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
