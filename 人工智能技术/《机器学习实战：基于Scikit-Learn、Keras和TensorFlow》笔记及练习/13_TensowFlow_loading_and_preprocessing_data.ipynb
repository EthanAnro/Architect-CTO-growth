{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ede3d7ec",
   "metadata": {},
   "source": [
    "## 一、数据API\n",
    "整个数据API都围绕着数据集的概念。\n",
    "\n",
    "**使用tf.data.Dataset.from_tensor_slices（）在RAM中完全创建一个数据集：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e59353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 17:13:14.262050: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 17:13:20.420365: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "X = tf.range(10) # numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc179dd",
   "metadata": {},
   "source": [
    "from_tensor_slices（）函数采用一个张量并创建一个tf.data.Dataset，其元素都是X的切片（沿第一个维度），因此此数据集包含10个元素：张量0，1，2，…，9。在这种情况下，如果我们使用tf.data.Dataset.range（10），则将获得相同的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8a40e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "260f5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b880cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb28f5",
   "metadata": {},
   "source": [
    "### 1、链式转换\n",
    "有了数据集后，我们可以通过调用其转换方法对其应用各种转换。每个方法都返回一个新的数据集，我们可以像这样进行链式转换:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c770da7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 18], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b23ad",
   "metadata": {},
   "source": [
    "> 数据集方法不会修改数据集，而是创建新数据集，因此请保留对这些新数据集的引用（例如，使用dataset=...），否则将不会发生任何事情。\n",
    "\n",
    "还可以通过调用map（）方法来变换元素。\n",
    "\n",
    "例如，这将创建一个新数据集，其中所有元素均是原来的两倍："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a714d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36b1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0  4  8 12 16 20 24], shape=(7,), dtype=int32)\n",
      "tf.Tensor([28 32 36  0  4  8 12], shape=(7,), dtype=int32)\n",
      "tf.Tensor([16 20 24 28 32 36  0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([ 4  8 12 16 20 24 28], shape=(7,), dtype=int32)\n",
      "tf.Tensor([32 36], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c4471",
   "metadata": {},
   "source": [
    "### 2、乱序数据\n",
    "当训练集中的实例相互独立且分布均匀时，梯度下降效果最佳。确保这一点的一种简单方法是使用shuffle（）方法对实例进行乱序。它会创建一个新的数据集，该数据集首先将源数据集的第一项元素填充到缓冲区中。然后无论任何时候要求提供一个元素，它都会从缓冲区中随机取出一个元素，并用源数据集中的新元素替换它，直到完全遍历源数据集为止。它将继续从缓冲区中随机抽取元素直到其为空。你必须指定缓冲区的大小，重要的是要使其足够大，否则乱序不会非常有效。不要超出你有的RAM的数量，即使你有足够的RAM，也不需要超出数据集的大小。如果每次运行程序都想要相同的随机顺序，你可以提供随机种子。\n",
    "\n",
    "例如，创建并显示一个包含整数0到9的数据集，重复3次，使用大小为5的缓冲区和42的随机种子进行乱序，并以7的批次大小进行批处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "827de1be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3bf03",
   "metadata": {},
   "source": [
    "对于不适合内存的大型数据集，这种简单的缓冲区乱序方法可能不够用，因为与数据集相比，缓冲区很小。\n",
    "\n",
    "一种常见的方法是将源数据拆分为多个文件，然后在训练过程中以随机顺序读取它们。但是位于同一文件中的实例仍然相互接近。为了避免这种情况，你可以随机选择多个文件并同时读取它们，并且交错它们的记录。然后最重要的是，你可以使用shuffle（）方法添加一个乱序缓冲区。\n",
    "\n",
    "虽然听起来这很麻烦，但是不用担心：Data API只需几行代码就可以实现所有这些功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bd80e",
   "metadata": {},
   "source": [
    "### Split the California dataset to multiple CSV files\n",
    "首先，假设已经加载了加州住房数据集，对其进行乱序，然后将其分为训练集、验证集和测试集。之后将每个集合分成许多类似如下的CSV文件（每行包含8个输入特征以及目标房屋中间值）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a8636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, \n",
    "    housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cea937",
   "metadata": {},
   "source": [
    "For a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel. To demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67ccef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.getcwd()\n",
    "    housing_dir = os.path.join(housing_dir, \"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70e9a8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c6047d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedianHouseValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5214</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.049945</td>\n",
       "      <td>1.106548</td>\n",
       "      <td>1447.0</td>\n",
       "      <td>1.605993</td>\n",
       "      <td>37.63</td>\n",
       "      <td>-122.43</td>\n",
       "      <td>1.442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.3275</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.490060</td>\n",
       "      <td>0.991054</td>\n",
       "      <td>3464.0</td>\n",
       "      <td>3.443340</td>\n",
       "      <td>33.69</td>\n",
       "      <td>-117.39</td>\n",
       "      <td>1.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.1000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.542373</td>\n",
       "      <td>1.591525</td>\n",
       "      <td>1328.0</td>\n",
       "      <td>2.250847</td>\n",
       "      <td>38.44</td>\n",
       "      <td>-122.98</td>\n",
       "      <td>1.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1736</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.289003</td>\n",
       "      <td>0.997442</td>\n",
       "      <td>1054.0</td>\n",
       "      <td>2.695652</td>\n",
       "      <td>33.55</td>\n",
       "      <td>-117.70</td>\n",
       "      <td>2.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0549</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.312457</td>\n",
       "      <td>1.085092</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>2.244384</td>\n",
       "      <td>33.93</td>\n",
       "      <td>-116.93</td>\n",
       "      <td>0.956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n",
       "1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n",
       "2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n",
       "3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n",
       "4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n",
       "\n",
       "   Longitude  MedianHouseValue  \n",
       "0    -122.43             1.442  \n",
       "1    -117.39             1.687  \n",
       "2    -122.98             1.621  \n",
       "3    -117.70             2.621  \n",
       "4    -116.93             0.956  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ce67fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n",
      "3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n",
      "7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
     ]
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5fff1ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/dayao/Github/Architect-CTO-growth/人工智能技术/《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》笔记及练习/datasets/housing/my_train_00.csv',\n",
       " '/Users/dayao/Github/Architect-CTO-growth/人工智能技术/《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》笔记及练习/datasets/housing/my_train_01.csv',\n",
       " '/Users/dayao/Github/Architect-CTO-growth/人工智能技术/《机器学习实战：基于Scikit-Learn、Keras和TensorFlow》笔记及练习/datasets/housing/my_train_02.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_filepaths[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca7acb",
   "metadata": {},
   "source": [
    "### Building an Input Pipeline\n",
    "创建一个仅包含以下文件路径的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03448757",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31da59e",
   "metadata": {},
   "source": [
    "调用interleave（）方法一次读取5个文件并交织它们的行（使用skip（）方法跳过每个文件的第一行，即标题行）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab17bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath:tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb910b3",
   "metadata": {},
   "source": [
    "interleave（）方法将创建一个数据集，该数据集将从filepath_dataset中拉出5个文件路径，对于每个路径，它将调用你为其提供的函数（在此示例中为lambda）来创建新的数据集（在此示例中为TextLineDataset）。为了清楚起见，在此阶段总共有7个数据集：文件路径数据集、交织数据集和由交织数据集在内部创建的5个TextLineDataset。当我们遍历交织数据集时，它将循环遍历这5个TextLineDatasets，每次读取一行，直到所有数据集都读出为止。然后它将从filepath_dataset获取剩下的5个文件路径，并以相同的方式对它们进行交织，以此类推，直到读完文件路径。\n",
    "\n",
    "随机选择的5个CSV文件的第一行（忽略标题行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "972f5386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.701388888888889,0.9965277777777778,692.0,2.4027777777777777,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea66e7b",
   "metadata": {},
   "source": [
    "### 预处理数据\n",
    "实现一个执行预处理的小函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05f81d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72e8f090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
       "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf16d61",
   "metadata": {},
   "source": [
    "## 二、TFRecord格式\n",
    "TFRecord格式是TensorFlow首选的格式，用于存储大量数据并有效读取数据。这是一种非常简单的二进制格式，只包含大小不同的二进制记录序列（每个记录由一个长度、一个用于检查长度是否损坏的CRC校验和、实际数据以及最后一个CRC校验和组成）。\n",
    "\n",
    "我们可以使用tf.io.TFRecordWriter类轻松创建TFRecord文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01bae6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"datasets/my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the forst record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e3bc9",
   "metadata": {},
   "source": [
    "然后我们可以使用tf.data.TFRecordDataset读取一个或多个TFRecord文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2828986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the forst record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"datasets/my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26426e",
   "metadata": {},
   "source": [
    "> 默认情况下，TFRecordDataset将一个接一个地读取文件，但是我们可以通过设置num_parallel_reads使其并行读取多个文件并交织记录。另外，我们可以使用list_files（）和interleave（）得到与前面读取多个CSV文件相同的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40283402",
   "metadata": {},
   "source": [
    "## 三、预处理输入特征（Data API）\n",
    "为神经网络准备数据需要将所有特征转换为数值特征，通常将其归一化等。特别是如果我们的数据包含分类特征或文本特征，则需要将它们转换为数字。在准备数据文件时，可以使用任何我们喜欢的工具（例如NumPy、pandas或Scikit-Learn）提前完成此操作。或者，可以在使用Data API加载数据时动态地预处理数据（例如使用数据集的map（）方法），也可以在模型中直接包含预处理层。现在让我们来看最后一个选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e99f1e",
   "metadata": {},
   "source": [
    "## 四、　TF Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "788b0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "\n",
    "    def preprocess(inputs):  # inputs is a batch of input features\n",
    "        median_age = inputs[\"housing_median_age\"]\n",
    "        ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
    "        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "        return {\n",
    "            \"standardized_median_age\": standardized_age,\n",
    "            \"ocean_proximity_id\": ocean_proximity_id\n",
    "        }\n",
    "except ImportError:\n",
    "    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38f11d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-transform\n",
      "  Downloading tensorflow_transform-1.10.1-py3-none-any.whl (439 kB)\n",
      "\u001b[K     |████████████████████████████████| 439 kB 63 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-transform) (1.23.3)\n",
      "Collecting apache-beam[gcp]<3,>=2.40\n",
      "  Downloading apache_beam-2.42.0-cp39-cp39-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 47 kB/s eta 0:00:01     |████████████▏                   | 1.8 MB 46 kB/s eta 0:01:04\n",
      "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-transform) (1.4.2)\n",
      "Collecting tfx-bsl<1.11.0,>=1.10.1\n",
      "  Downloading tfx_bsl-1.10.1-cp39-cp39-macosx_10_14_x86_64.whl (22.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.9 MB 66 kB/s eta 0:00:014    |████▉                           | 3.4 MB 91 kB/s eta 0:03:34\n",
      "\u001b[?25hCollecting tensorflow-metadata<1.11.0,>=1.10.0\n",
      "  Downloading tensorflow_metadata-1.10.0-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 93 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-transform) (1.2.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-transform) (3.19.6)\n",
      "Collecting pyarrow<7,>=6\n",
      "  Downloading pyarrow-6.0.1-cp39-cp39-macosx_10_13_x86_64.whl (19.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 19.2 MB 106 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5\n",
      "  Downloading tensorflow-2.9.2-cp39-cp39-macosx_10_14_x86_64.whl (228.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 228.6 MB 8.5 kB/s ta 0:00:019    |████▊                           | 34.0 MB 66 kB/s eta 0:48:47     |███████▊                        | 54.9 MB 79 kB/s eta 0:36:38     |██████████▉                     | 77.7 MB 69 kB/s eta 0:36:03     |███████████████▌                | 111.0 MB 72 kB/s eta 0:26:55     |███████████████████████         | 165.1 MB 81 kB/s eta 0:13:04     |██████████████████████████      | 186.0 MB 93 kB/s eta 0:07:35     |███████████████████████████████▊| 226.7 MB 100 kB/s eta 0:00:19\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pymongo/\u001b[0m\n",
      "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.13.0-cp39-cp39-macosx_10_9_universal2.whl (430 kB)\n",
      "\u001b[K     |████████████████████████████████| 430 kB 82 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2.26.0)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 107 kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cloudpickle~=2.1.0\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.8.1-cp39-cp39-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (490 kB)\n",
      "\u001b[K     |████████████████████████████████| 490 kB 73 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (1.49.1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2021.8.3)\n",
      "Collecting zstandard<1,>=0.18.0\n",
      "  Downloading zstandard-0.19.0-cp39-cp39-macosx_10_9_x86_64.whl (453 kB)\n",
      "\u001b[K     |████████████████████████████████| 453 kB 95 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 78 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (3.10.0.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2021.3)\n",
      "Collecting numpy<2,>=1.16\n",
      "  Downloading numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 69 kB/s eta 0:00:017    |███████████▍                    | 6.3 MB 93 kB/s eta 0:02:02\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2.8.2)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.7.0-cp39-cp39-macosx_10_15_x86_64.whl (529 kB)\n",
      "\u001b[K     |████████████████████████████████| 529 kB 80 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 74 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 120 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigquery<3,>=1.6.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 88 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 122 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0\n",
      "  Downloading google_cloud_pubsublite-1.6.0-py2.py3-none-any.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 91 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0\n",
      "  Downloading google_cloud_dlp-3.9.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 89 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5,>=3.1.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting google-cloud-datastore<2,>=1.8.0\n",
      "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 78 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.2-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[K     |████████████████████████████████| 267 kB 89 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[K     |████████████████████████████████| 435 kB 78 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2.12.0)\n",
      "Collecting google-cloud-core<3,>=0.28.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 149 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-httplib2<0.2.0,>=0.1.0\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting grpcio-gcp<1,>=0.2.2\n",
      "  Downloading grpcio_gcp-0.2.2-py2.py3-none-any.whl (9.4 kB)\n",
      "Collecting google-cloud-bigquery-storage<2.14,>=2.6.3\n",
      "  Downloading google_cloud_bigquery_storage-2.13.2-py2.py3-none-any.whl (180 kB)\n",
      "\u001b[K     |████████████████████████████████| 180 kB 120 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-recommendations-ai<0.8.0,>=0.1.0\n",
      "  Downloading google_cloud_recommendations_ai-0.7.1-py2.py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 80 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[K     |████████████████████████████████| 173 kB 75 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-pubsub<3,>=2.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_cloud_pubsub-2.13.10-py2.py3-none-any.whl (236 kB)\n",
      "\u001b[K     |████████████████████████████████| 236 kB 80 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core!=2.8.2,<3\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "\u001b[K     |████████████████████████████████| 115 kB 80 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 89 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 97 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fasteners>=0.14\n",
      "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
      "Collecting oauth2client>=1.4.12\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 88 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (4.9)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 128 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging<22.0dev,>=14.3 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (21.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Collecting overrides<7.0.0,>=6.0.1\n",
      "  Downloading overrides-6.5.0-py3-none-any.whl (17 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (30 kB)\n",
      "Collecting grpcio!=1.48.0,<2,>=1.33.1\n",
      "  Downloading grpcio-1.50.0-cp39-cp39-macosx_10_10_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 99 kB/s eta 0:00:018\n",
      "\u001b[?25hCollecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from httplib2<0.21.0,>=0.8->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (3.0.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.40->tensorflow-transform) (1.26.7)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 92 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (1.12.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (3.2.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (2.0.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (58.0.4)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 101 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.4.0)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.27.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 76 kB/s eta 0:00:014\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (0.4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (4.8.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: zipp>=0.5 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5->tensorflow-transform) (3.2.1)\n",
      "Collecting google-api-python-client<2,>=1.7.11\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 70 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tfx-bsl<1.11.0,>=1.10.1->tensorflow-transform) (1.3.4)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.10.0-py2.py3-none-any.whl (37 kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.9.2-py2.py3-none-any.whl (37 kB)\n",
      "Building wheels for collected packages: crcmod, dill, google-apitools, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-macosx_10_9_x86_64.whl size=22091 sha256=8cae2013a7cc997e6a36c7992513d4cd043f618b4974df3b9c94ae4b4bd6b322\n",
      "  Stored in directory: /Users/dayao/Library/Caches/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=478128e58d383c2187ada850d68252144a6d7a4cc150c225aee08aaf3d67d37d\n",
      "  Stored in directory: /Users/dayao/Library/Caches/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131040 sha256=3030b4cfce0a86c6ff94ffdeb87295faf924bdc7256fdd00ca77fedabe4113fb\n",
      "  Stored in directory: /Users/dayao/Library/Caches/pip/wheels/6c/f8/60/b9e91899dbaf25b6314047d3daee379bdd8d61b1dc3fd5ec7f\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=e98f59c546e789e460aac4067afafda26db3794cfe7766f0d9b54af612fc6532\n",
      "  Stored in directory: /Users/dayao/Library/Caches/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built crcmod dill google-apitools docopt\n",
      "Installing collected packages: cachetools, googleapis-common-protos, grpcio-status, google-api-core, proto-plus, numpy, httplib2, grpcio-gcp, grpc-google-iam-v1, google-crc32c, docopt, zstandard, tensorflow-estimator, tensorboard, pymongo, pyarrow, overrides, orjson, oauth2client, keras, hdfs, google-resumable-media, google-cloud-pubsub, google-cloud-core, flatbuffers, fasteners, fastavro, dill, crcmod, cloudpickle, uritemplate, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsublite, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-bigquery-storage, google-cloud-bigquery, google-auth-httplib2, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, google-api-python-client, tfx-bsl, tensorflow-transform\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.3\n",
      "    Uninstalling numpy-1.23.3:\n",
      "      Successfully uninstalled numpy-1.23.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.10.0\n",
      "    Uninstalling tensorflow-estimator-2.10.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.10.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.10.1\n",
      "    Uninstalling tensorboard-2.10.1:\n",
      "      Successfully uninstalled tensorboard-2.10.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 22.9.24\n",
      "    Uninstalling flatbuffers-22.9.24:\n",
      "      Successfully uninstalled flatbuffers-22.9.24\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.10.0\n",
      "    Uninstalling tensorflow-2.10.0:\n",
      "      Successfully uninstalled tensorflow-2.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\u001b[0m\n",
      "Successfully installed apache-beam-2.42.0 cachetools-4.2.4 cloudpickle-2.1.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.7.0 fasteners-0.18 flatbuffers-1.12 google-api-core-2.10.2 google-api-python-client-1.12.11 google-apitools-0.5.31 google-auth-httplib2-0.1.0 google-cloud-bigquery-2.34.4 google-cloud-bigquery-storage-2.13.2 google-cloud-bigtable-1.7.2 google-cloud-core-2.3.2 google-cloud-datastore-1.15.5 google-cloud-dlp-3.9.2 google-cloud-language-1.3.2 google-cloud-pubsub-2.13.10 google-cloud-pubsublite-1.6.0 google-cloud-recommendations-ai-0.7.1 google-cloud-spanner-1.19.3 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 google-crc32c-1.5.0 google-resumable-media-2.4.0 googleapis-common-protos-1.56.4 grpc-google-iam-v1-0.12.4 grpcio-gcp-0.2.2 grpcio-status-1.48.2 hdfs-2.7.0 httplib2-0.20.4 keras-2.9.0 numpy-1.22.4 oauth2client-4.1.3 orjson-3.8.1 overrides-6.5.0 proto-plus-1.22.1 pyarrow-6.0.1 pymongo-3.13.0 tensorboard-2.9.1 tensorflow-2.9.2 tensorflow-estimator-2.9.0 tensorflow-metadata-1.10.0 tensorflow-serving-api-2.9.2 tensorflow-transform-1.10.1 tfx-bsl-1.10.1 uritemplate-3.0.1 zstandard-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U tensorflow-transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3649c83",
   "metadata": {},
   "source": [
    "**借助Data API、TFRecords、Keras预处理层和TF Transform，你可以构建高度可扩展的输入流水线来进行训练，并从生产环境中的快速而便携的数据预处理中受益。**但是如果你只想使用标准数据集怎么办？在这种情况下，事情要简单得多：只需使用TFDS！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6418e15d",
   "metadata": {},
   "source": [
    "## 五、TensorFlow数据集项目\n",
    "TensorFlow数据集（TFDS）项目使下载通用数据集变得非常容易，从小型数据集（如MNIST或Fashion MNIST）到大型数据集（如ImageNet）等。\n",
    "TFDS没有与TensorFlow捆绑在一起，因此我们需要安装tensorflow_datasets库。然后调用tfds.load（）函数，它会下载你想要的数据（除非之前已经下载过），并将该数据作为数据集的目录返回（通常一个用于训练，另一个用于测试，但这取决于你选择的数据集）。\n",
    "\n",
    "例如，让我们下载MNIST："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61f4cf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (3.19.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Collecting etils[epath]\n",
      "  Downloading etils-0.9.0-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (4.62.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.10.0)\n",
      "Requirement already satisfied: absl-py in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.2.0)\n",
      "Requirement already satisfied: dill in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (0.3.1.1)\n",
      "Requirement already satisfied: numpy in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.22.4)\n",
      "Requirement already satisfied: termcolor in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (2.0.1)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: zipp in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from etils[epath]->tensorflow_datasets) (3.6.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from etils[epath]->tensorflow_datasets) (3.10.0.2)\n",
      "Collecting importlib_resources\n",
      "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/dayao/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21502 sha256=90c2e073f976b676ab907190bf0611437a2b87b96c52fa3fc75ed7f9ceb2e7c0\n",
      "  Stored in directory: /Users/dayao/Library/Caches/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: etils, importlib-resources, promise, tensorflow-datasets\n",
      "Successfully installed etils-0.9.0 importlib-resources-5.10.0 promise-2.3 tensorflow-datasets-4.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c80f13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 17:33:25.764144: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "2022-11-08 17:34:26.770296: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.007877 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:35:27.920599: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002701 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:36:29.757524: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003089 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:37:31.478443: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.028135 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:38:33.498050: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbead82cd30 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.001411 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:39:36.700635: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002428 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:40:41.040143: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.00199 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:41:48.789036: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003257 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:43:03.495276: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9add23a0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.00738 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:44:30.430341: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.00201 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:46:04.369726: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9add23a0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/dataset_info%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003174 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /Users/dayao/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-08 17:47:05.484065: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9ae98d60 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.00165 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:48:07.558776: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbead82cd30 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.013479 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:49:09.119262: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003607 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:50:10.589512: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbeadc10930 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002955 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:51:12.722460: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbeabe2d9c0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002446 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:52:16.282788: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002951 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:54:27.385885: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9add23a0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.002026 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:55:41.294813: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9d38df10 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.008574 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:57:08.134454: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbe9ae98d60 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.007101 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n",
      "2022-11-08 17:58:41.511344: E tensorflow/core/platform/cloud/curl_http_request.cc:614] The transmission  of request 0x7fbea06ceea0 (URI: https://www.googleapis.com/storage/v1/b/tfds-data/o/datasets%2Fmnist%2F3.0.1?fields=size%2Cgeneration%2Cupdated) has been stuck at 0 of 0 bytes for 61 seconds and will be aborted. CURL timing information: lookup time: 0.003275 (No error), connect time: 0 (No error), pre-transfer time: 0 (No error), start-transfer time: 0 (No error)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d091fdcb6e545a0961b01dcedce963a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c70ba7cc8c649bd89130d13c5f284a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883894dd00c84eed8b362f3ec36e83a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/dayao/tensorflow_datasets/mnist/3.0.1.incompleteAAB2JU/mnist-train.tfrecord*...:   0%|       …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/dayao/tensorflow_datasets/mnist/3.0.1.incompleteAAB2JU/mnist-test.tfrecord*...:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/dayao/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
