{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc308669",
   "metadata": {},
   "source": [
    "所有计算机程序最终都可以简化为对二进制输入的一些二进制运算（AND、OR、NOR等），与此类似，深度神经网络学到的所有变换也都可以简化为对数值数据张量的一些张量运算（tensor operation）或张量函数（tensor function），如张量加法、张量乘法等。\n",
    "\n",
    "> 如我们通过使用Keras叠加Dense层来构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e825ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # 忽略警告信息，不影响代码执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e22b8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x7fbd32763ca0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3fa5d9",
   "metadata": {},
   "source": [
    "可以将这个层理解为一个函数，其输入是一个矩阵，返回的是另一个矩阵，即输入张量的新表示。这个函数具体如下（其中W是一个矩阵，b是一个向量，二者都是该层的属性）\n",
    "\n",
    "``output = relu(dot(input, W) + b)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd5a93",
   "metadata": {},
   "source": [
    "将上式拆开来看。这里有3个张量运算。\n",
    "1. 输入张量和张量W之间的点积运算（dot）。\n",
    "2. 由此得到的矩阵与向量b之间的加法运算（+）。\n",
    "3. relu运算。relu(x)就是max(x, 0)，relu代表“修正线性单元”（rectified linear unit）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc036ed",
   "metadata": {},
   "source": [
    "## 2.3.1　逐元素运算\n",
    "relu运算和加法都是逐元素（element-wise）运算，即该运算分别应用于张量的每个元素。也就是说，这些运算非常适合大规模并行实现（向量化实现，这一术语来自于1970～1990年间向量处理器超级计算机架构）。如果想对逐元素运算编写一个简单的Python实现，那么可以使用for循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e943a276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对逐元素relu运算的简单实现\n",
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2  # x是一个2阶NumPy张量\n",
    "    x = x.copy()  # 避免覆盖输入张量\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "# 加法运算\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2  # x和y是2阶NumPy张量\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()  # 避免覆盖输入张量\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360d235",
   "metadata": {},
   "source": [
    "在实践中处理NumPy数组时，这些运算都是优化好的NumPy内置函数。这些函数将大量运算交给基础线性代数程序集（Basic Linear Algebra Subprograms，BLAS）实现。BLAS是低层次（low-level）、高度并行、高效的张量操作程序，通常用Fortran或C语言来实现。因此，在NumPy中可以直接进行下列逐元素运算，速度非常快。来看一下两种方法运行时间的差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21af46ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.01 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))\n",
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = x + y\n",
    "    z = np.maximum(z, 0.)\n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ac0597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "for _ in range(1000):\n",
    "    z = naive_add(x, y)\n",
    "    z = naive_relu(z)\n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de2522",
   "metadata": {},
   "source": [
    "## 2.3.2　广播\n",
    "2.3.1节对naive_add的简单实现仅支持两个形状相同的2阶张量相加。如果将两个形状不同的张量相加，会发生什么？\n",
    "\n",
    "在没有歧义且可行的情况下，较小的张量会被广播（broadcast），以匹配较大张量的形状。广播包含以下两步。\n",
    "1. 向较小张量添加轴[叫作广播轴（broadcast axis）]，使其ndim与较大张量相同。\n",
    "2. 将较小张量沿着新轴重复，使其形状与较大张量相同。\n",
    "\n",
    "## 2.3.3　张量积\n",
    "张量积（tensor product）或点积（dot product）是最常见且最有用的张量运算之一。\n",
    "\n",
    "## 2.3.4　张量变形\n",
    "张量变形是指重新排列张量的行和列，以得到想要的形状。变形后，张量的元素个数与初始张量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15ea60ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[0., 1.],\n",
    "                  [2., 3.],\n",
    "                  [4., 5.]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef31fb7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec1a7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03584954",
   "metadata": {},
   "source": [
    "张量转置（transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22a1c48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((300, 20)) # 一个形状为(300, 20)的零矩阵\n",
    "x = np.transpose(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f86db30",
   "metadata": {},
   "source": [
    "## 2.3.5　张量运算的几何解释\n",
    "对于张量运算所操作的张量，其元素可看作某个几何空间中的点的坐标，因此所有的张量运算都有几何解释。\n",
    "\n",
    "## 2.3.6　深度学习的几何解释\n",
    "前面说过，神经网络完全由一系列张量运算组成，而这些张量运算只是输入数据的简单几何变换。因此，你可以将神经网络解释为高维空间中非常复杂的几何变换，这种变换通过一系列简单步骤来实现。\n",
    "\n",
    "对于三维的情况，下面这个思维模型是很有用的。想象有两张彩纸：一张红色，一张蓝色。将一张纸放在另一张上，然后把它们一起揉成一个小球。这个皱巴巴的纸团就是你的输入数据，每张纸对应分类问题中的一个数据类别。神经网络要做的就是，找到可以让纸团恢复平整的变换，从而再次让两个类别明确可分（见下图）。利用深度学习，这一过程可以通过三维空间中一系列简单变换来实现，比如用手指对纸团所做的变换，每次一个动作。\n",
    "![解开复杂的数据流形](image/解开复杂的数据流形.png)\n",
    "\n",
    "让纸团恢复平整就是机器学习的目的：为高维空间中复杂、高度折叠的数据流形（manifold）找到简洁的表示。流形是指一个连续的表面，比如揉皱的纸。\n",
    "\n",
    "深度学习特别擅长这一点：它可以将复杂的几何变换逐步分解为一系列基本变换，这与我们展开纸团所采取的策略大致相同。深度神经网络的每一层都通过变换使数据解开一点点，而许多层堆叠在一起，可以实现极其复杂的解开过程。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
